{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI - Request for Research 2.0\n",
    "## Warmup 1\n",
    "https://blog.openai.com/requests-for-research-2/\n",
    "\n",
    "Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâ€™s end. Test the two approaches below:\n",
    "* Generate a dataset of random 100,000 binary strings of length 50. Train the LSTM; what performance do you get?\n",
    "* Generate a dataset of random 100,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "batch_size = 128\n",
    "training_size_v1 = 100000\n",
    "training_size_v2 = 10000\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_example(training_size=100000, str_len=50):\n",
    "    \"\"\"\n",
    "    1 << n is the first number with n+1 digits\n",
    "    in other words, (1 << n) - 1 is the last number with n digits\n",
    "    \"\"\"\n",
    "    low = 0\n",
    "    high = (1 << str_len)\n",
    "    for __ in range(training_size):\n",
    "        num = np.random.randint(low, high)\n",
    "        bits = np.binary_repr(num, width=str_len)\n",
    "        \n",
    "        X = np.zeros((str_len, 2))\n",
    "        Y = np.zeros((str_len, 2))\n",
    "        \n",
    "        parity = 0\n",
    "        \n",
    "        for i, bit in enumerate(bits):\n",
    "            parity ^= int(bit)\n",
    "            X[i, int(bit)] = 1\n",
    "            Y[i, parity] = 1\n",
    "        \n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_v1(training_size=100000, string_len=50, padding=True):\n",
    "    np.random.seed(0)\n",
    "    inputs = np.zeros((training_size, string_len, 2))\n",
    "    outputs = np.zeros((training_size, string_len, 2))\n",
    "    for i, (X, Y) in enumerate(gen_training_example(training_size, string_len)):\n",
    "        inputs[i,:,:] = X\n",
    "        outputs[i,:,:] = Y\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_v2(training_size=10000, string_len=50, padding=False):\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for __ in range(training_size):\n",
    "        seq_len = np.random.randint(1, string_len + 1)        \n",
    "        inputs = np.zeros((1, seq_len, 2))\n",
    "        outputs = np.zeros((1, seq_len, 2))\n",
    "        \n",
    "        for j, (X, Y) in enumerate(gen_training_example(1, seq_len)):\n",
    "            inputs[j,:,:] = X\n",
    "            outputs[j,:,:] = Y\n",
    "        yield inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_Model(input_shape):\n",
    "    binary_string = Input(shape=input_shape, dtype='float32')\n",
    "    X = LSTM(1, kernel_initializer='glorot_normal', return_sequences=True)(binary_string)\n",
    "    X = Dense(2, activation=K.softmax)(X)\n",
    "    X = Activation(K.softmax)(X)\n",
    "    \n",
    "    model = Model(inputs=[binary_string], outputs=[X])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Fixed Training Size\n",
    "A basic LSTM (hidden unit size 2) is trained with fixed training input sequence length of 50.\n",
    "This model achieves 100% accuracy within 6-8 epochs of training with batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50, 2)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 1)             16        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50, 2)             4         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50, 2)             0         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = XOR_Model(input_shape=(max_len, 2))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (100000, 50, 2) // output shape: (100000, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_train, Y_train = load_data_v1(training_size_v1, max_len, True)\n",
    "X_test, Y_test = load_data_v1(test_size, max_len, True)\n",
    "print('input shape: {} // output shape: {}'.format(X_train.shape, Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/7\n",
      "100000/100000 [==============================] - 80s 799us/step - loss: 0.6930 - acc: 0.5063 - val_loss: 0.6927 - val_acc: 0.5086\n",
      "Epoch 2/7\n",
      "100000/100000 [==============================] - 79s 792us/step - loss: 0.6899 - acc: 0.5185 - val_loss: 0.6852 - val_acc: 0.5356\n",
      "Epoch 3/7\n",
      "100000/100000 [==============================] - 78s 784us/step - loss: 0.5487 - acc: 0.7924 - val_loss: 0.3898 - val_acc: 1.0000\n",
      "Epoch 4/7\n",
      "100000/100000 [==============================] - 77s 768us/step - loss: 0.3674 - acc: 1.0000 - val_loss: 0.3519 - val_acc: 1.0000\n",
      "Epoch 5/7\n",
      "100000/100000 [==============================] - 76s 762us/step - loss: 0.3437 - acc: 1.0000 - val_loss: 0.3369 - val_acc: 1.0000\n",
      "Epoch 6/7\n",
      "100000/100000 [==============================] - 75s 755us/step - loss: 0.3325 - acc: 1.0000 - val_loss: 0.3286 - val_acc: 1.0000\n",
      "Epoch 7/7\n",
      "100000/100000 [==============================] - 76s 757us/step - loss: 0.3259 - acc: 1.0000 - val_loss: 0.3236 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a70ab5a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should perform close to 100%\n",
    "model.fit(X_train, Y_train, epochs=7, validation_data=(X_test, Y_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Variable Training Size\n",
    "The same model is trained, but with variable training input sequence length between 1 and 50 inclusive (uniform distribution).\n",
    "Randomly selected input doesn't allow us to take advantage of vectorization, and therefore this model is trained with stochastic gradient descent with epoch size of 5 for each example.\n",
    "This model achieves 100% accuracy in both training/test sets after training on ~2000 examples with 5 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None, 2)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 1)           16        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 2)           4         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, None, 2)           0         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v2 = XOR_Model(input_shape=(None, 2))\n",
    "model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 examples trained (each with 5 epoch)\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 0.6906 - acc: 0.5556\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6905 - acc: 0.5556\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6904 - acc: 0.5333\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6903 - acc: 0.5333\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6902 - acc: 0.5556\n",
      "2001 examples trained (each with 5 epoch)\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3149 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3149 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3149 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3149 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3149 - acc: 1.0000\n",
      "4001 examples trained (each with 5 epoch)\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3133 - acc: 1.0000\n",
      "6001 examples trained (each with 5 epoch)\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3133 - acc: 1.0000\n",
      "8001 examples trained (each with 5 epoch)\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3133 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Hack to handle variable length sequence in arbitrary order\n",
    "for i, (X_train_v2, Y_train_v2) in enumerate(load_data_v2(training_size_v2, max_len, False)):\n",
    "    if i % 2000 == 0:\n",
    "        print('{} examples trained (each with 5 epoch)'.format(i+1))\n",
    "        model_v2.fit(X_train_v2, Y_train_v2, epochs=5, batch_size=1)\n",
    "    else:\n",
    "        model_v2.fit(X_train_v2, Y_train_v2, epochs=5, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 734us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3132616877555847, 1.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v2.evaluate(X_test, Y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Layer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Arbitrary Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, test_size)\n",
    "x = X_test[i]\n",
    "y = Y_test[i]\n",
    "x = x.reshape((1, max_len, 2))\n",
    "print('y: {} // y_hat: {}'.format(y[0],model.predict(x)[0,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
